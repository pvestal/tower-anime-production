# Anime Studio v3.2 Architecture

## System Overview

```mermaid
graph TB
    subgraph Browser
        UI[Vue 3 Frontend<br/>7 tabs + Echo Brain panel]
    end

    subgraph "Anime Studio (port 8401)"
        APP[src/app.py<br/>FastAPI entry point]

        subgraph "packages/"
            CORE[core/<br/>DB, auth, config, GPU]
            STORY[story/<br/>15 routes]
            VIS[visual_pipeline/<br/>5 routes]
            SCENE[scene_generation/<br/>19 routes]
            EPISODE[episode_assembly/<br/>10 routes]
            TRAIN[lora_training/<br/>32 routes]
            AUDIO[audio_composition/<br/>4 routes]
            ECHO_PKG[echo_integration/<br/>4 routes]
        end

        APP --> CORE
        APP --> STORY
        APP --> VIS
        APP --> SCENE
        APP --> EPISODE
        APP --> TRAIN
        APP --> AUDIO
        APP --> ECHO_PKG
    end

    subgraph "External Services"
        ComfyUI[ComfyUI<br/>port 8188<br/>NVIDIA RTX 3060]
        Ollama[Ollama<br/>port 11434<br/>Gemma3:12b]
        Echo[Echo Brain<br/>port 8309<br/>AMD RX 9070 XT]
        Jellyfin[Jellyfin<br/>port 8096<br/>media server]
        Qdrant[Qdrant<br/>port 6333<br/>54K+ vectors]
        PG[(PostgreSQL<br/>anime_production)]
        Vault[Vault<br/>DB credentials]
    end

    UI -->|REST API| APP
    UI -->|WebSocket| ComfyUI
    VIS --> ComfyUI
    VIS --> Ollama
    SCENE --> ComfyUI
    SCENE --> Ollama
    EPISODE --> Jellyfin
    ECHO_PKG --> Echo
    CORE --> PG
    CORE --> Vault
    Echo --> Qdrant
```

## Package Structure

```
packages/
  core/               # Shared infrastructure + autonomy
    __init__.py
    auth.py           # JWT auth middleware (local bypass, external JWT)
    config.py         # Paths, URLs, model names
    db.py             # asyncpg pool, migrations, character cache
    gpu_router.py     # GPU/Ollama status endpoint
    models.py         # Pydantic request/response models
    events.py         # EventBus — async event emitter
    learning.py       # Pattern analysis from gen history/rejections/approvals
    audit.py          # Autonomy decision logging
    model_selector.py # Param recommendations, drift detection, learned negatives
    auto_correction.py # 7 fix strategies for rejected images
    replenishment.py  # Autonomous generation loop (target-based)

  story/              # 15 routes
    router.py         # Project CRUD, storyline, world settings, generation styles

  visual_pipeline/    # 5 routes
    router.py         # Vision review endpoint (Gemma3 auto-triage)
    classification.py # Character identification, confusable pair verification
    vision.py         # Image quality assessment, species verification, perceptual hash
    comfyui.py        # Workflow building, submission, progress tracking

  scene_generation/   # 19 routes
    router.py         # Scene/shot CRUD, generate, assemble, video serve, motion presets, story-to-scenes
    builder.py        # Background generation task, continuity chaining, audio mixing
    framepack.py      # FramePack workflow building, I2V pipeline, motion presets
    story_to_scenes.py # AI scene breakdown from storyline (Ollama gemma3:12b)

  episode_assembly/   # 10 routes
    router.py         # Episode CRUD, scene linking, assemble, video serve, publish
    builder.py        # ffmpeg concat for multi-scene episodes, thumbnail extraction
    publish.py        # Jellyfin directory structure, symlinks, library scan

  lora_training/      # 32 routes
    router.py         # Dataset browsing, approval, gallery, checkpoints
    training_router.py # Training jobs, regeneration
    ingest_router.py  # Image/video upload, ComfyUI scan, IP-Adapter refine
    feedback.py       # Rejection feedback loop, negative prompt building

  audio_composition/  # 4 routes
    router.py         # Audio analysis, silence detection

  echo_integration/   # 4 routes
    router.py         # Echo Brain chat, prompt enhancement
```

## Database Schema (Key Tables)

```mermaid
erDiagram
    projects ||--o{ characters : has
    projects ||--o| generation_styles : "default_style"
    projects ||--o| storylines : has
    projects ||--o| world_settings : has
    projects ||--o{ scenes : has
    projects ||--o{ episodes : has

    scenes ||--o{ shots : contains
    episodes ||--o{ episode_scenes : contains
    scenes ||--o{ episode_scenes : "referenced by"

    projects {
        int id PK
        string name
        string description
        string genre
        string default_style FK
        string premise
        string content_rating
    }

    generation_styles {
        string style_name PK
        string checkpoint_model
        float cfg_scale
        int steps
        string sampler
        int width
        int height
        string positive_prompt_template
        string negative_prompt_template
    }

    characters {
        int id PK
        string name
        text design_prompt
        int project_id FK
        jsonb appearance_data
    }

    scenes {
        uuid id PK
        int project_id FK
        int scene_number
        string title
        text description
        string location
        string time_of_day
        string weather
        string mood
        string generation_status
        int target_duration_seconds
        float actual_duration_seconds
        string final_video_path
        int total_shots
        int completed_shots
    }

    shots {
        uuid id PK
        uuid scene_id FK
        int shot_number
        string shot_type
        string camera_angle
        float duration_seconds
        text motion_prompt
        string source_image_path
        string first_frame_path
        string last_frame_path
        string output_video_path
        string status
        int seed
        int steps
        bool use_f1
        float generation_time_seconds
        text dialogue_text
        string dialogue_character_slug
    }

    episodes {
        uuid id PK
        int project_id FK
        int episode_number
        string title
        text description
        string story_arc
        string status
        string final_video_path
        string thumbnail_path
        float actual_duration_seconds
    }

    episode_scenes {
        uuid episode_id FK
        uuid scene_id FK
        int position
        string transition
    }
```

## Character Quality Pipeline

```mermaid
graph TD
    subgraph "Generation"
        DB_CHAR[DB: appearance_data] --> NEG_BUILD[build_character_negatives]
        NEG_BUILD --> |"human, human face..."| NEG[Per-Character Negative Prompt]
        DB_STYLE[DB: generation_style] --> STYLE_NEG[Style Negative Template]
        STYLE_NEG --> COMBINED[Combined Negative Prompt]
        NEG --> COMBINED

        REF[reference_images/] --> IPA[IP-Adapter Injection<br/>weight 0.7]
        DESIGN[design_prompt] --> POS[Positive Prompt]

        POS --> COMFY[ComfyUI Generation]
        COMBINED --> COMFY
        IPA --> COMFY
    end

    subgraph "Vision Review (Gemma3)"
        COMFY --> IMG[Generated Image]
        IMG --> REVIEW[vision_review_image]
        DB_CHAR --> |species, key_colors,<br/>common_errors| CHECKLIST[build_feature_checklist]
        CHECKLIST --> REVIEW

        REVIEW --> SCORE{quality_score}
        SCORE --> |"< 0.4"| REJECT[Auto-Reject]
        SCORE --> |">= 0.8 + solo"| HUMAN_CHECK{Is species human?}
        SCORE --> |"0.4 - 0.8"| MANUAL[Manual Review]

        HUMAN_CHECK --> |Yes| APPROVE[Auto-Approve]
        HUMAN_CHECK --> |"No (NOT human)"| MANUAL
    end

    subgraph "Feedback Loop"
        REJECT --> FEEDBACK[feedback.json]
        FEEDBACK --> REGEN[Regenerate with<br/>additional negatives]
        REGEN --> COMFY
    end

    APPROVE --> TRAIN[LoRA Training]
    MANUAL --> |Human approval| TRAIN
```

## Replenishment Loop (Autonomous)

```mermaid
graph TD
    subgraph "EventBus Trigger"
        APPROVE_EVT[IMAGE_APPROVED Event] --> CHECK{Approved count<br/>< target?}
        CHECK --> |Yes| GUARDS{Safety checks:<br/>cooldown, daily limit,<br/>concurrent, pending buffer}
        CHECK --> |"No (at target)"| DONE[Stop]
        GUARDS --> |Pass| GEN[Generate batch<br/>3 images via<br/>generate_training_images.py]
        GUARDS --> |"Fail (rate limited)"| WAIT[Wait for next event]
    end

    subgraph "Autonomous Pipeline"
        GEN --> COMFY[ComfyUI Generation]
        COMFY --> COPY[Copy to datasets/slug/images/]
        COPY --> REGISTER[Register as pending<br/>in approval_status.json]
        REGISTER --> VISION[Vision Review<br/>Gemma3 auto-triage]
        VISION --> SCORE{quality_score}
        SCORE --> |">= 0.8 + solo"| AUTO_APPROVE[Auto-Approve]
        SCORE --> |"< 0.4"| AUTO_REJECT[Auto-Reject]
        SCORE --> |"0.4 - 0.8"| MANUAL_Q[Pending Manual Review]
    end

    subgraph "Learning & Loop"
        AUTO_APPROVE --> LEARN_S[Record success pattern]
        AUTO_REJECT --> LEARN_F[Record failure pattern]
        LEARN_S --> APPROVE_EVT
        LEARN_F --> |"5+ consecutive"| PAUSE[Pause character]
    end

    subgraph "Audit"
        GEN --> LOG[autonomy_decisions table]
        AUTO_APPROVE --> LOG
        AUTO_REJECT --> LOG
    end
```

## Species Verification

Non-human characters (species contains "NOT human") get additional safeguards:

1. **Per-character negative prompts**: `human, human face, human skin, realistic person` added automatically
2. **Species-specific negatives**: star-shaped creatures get `child, boy, girl, humanoid`; mushroom creatures get `human child, boy wearing hat`
3. **Species verification step**: Focused binary question about creature-specific visual traits (shell, scales, fur, etc.)
4. **Auto-approve blocked**: Non-human characters always require manual approval regardless of vision scores

Species checks defined for: turtle, koopa, dinosaur, star-shaped, mushroom, mouse, dragon.

## Scene Builder Pipeline

```mermaid
sequenceDiagram
    participant User
    participant Frontend as Vue Frontend
    participant API as FastAPI
    participant DB as PostgreSQL
    participant ComfyUI

    User->>Frontend: Create scene + add shots
    Frontend->>API: POST /scenes (create)
    API->>DB: INSERT scene
    Frontend->>API: POST /scenes/{id}/shots (per shot)
    API->>DB: INSERT shots

    User->>Frontend: Click "Generate"
    Frontend->>API: POST /scenes/{id}/generate
    API->>DB: Set status = "generating"
    API-->>Frontend: {total_shots, estimated_minutes}

    Note over API: Background asyncio task

    loop For each shot (ordered)
        API->>DB: Set shot status = "generating"

        alt First shot
            API->>API: Copy source_image to ComfyUI/input/
        else Subsequent shots
            API->>API: Use previous shot's last_frame
        end

        API->>ComfyUI: POST /prompt (FramePack workflow)
        ComfyUI-->>API: {prompt_id}

        loop Poll every 5s
            API->>ComfyUI: GET /history/{prompt_id}
        end

        ComfyUI-->>API: Completed + output files

        API->>API: ffmpeg extract last frame
        API->>DB: Set shot completed + paths
    end

    API->>API: ffmpeg concat all shot videos
    API->>DB: Set scene completed + final_video_path

    loop Poll every 5s
        Frontend->>API: GET /scenes/{id}/status
        API->>DB: Query scene + shot statuses
        API-->>Frontend: {generation_status, shots[]}
    end

    User->>Frontend: Play assembled video
    Frontend->>API: GET /scenes/{id}/video
    API-->>Frontend: MP4 FileResponse
```

## Frontend Component Tree

```mermaid
graph TD
    APP[App.vue<br/>nav + router-view] --> PROJ[ProjectTab]
    APP --> CHAR[CharactersTab]
    APP --> CREATE[CreateTab]
    APP --> REVIEW[ReviewTab]
    APP --> TRAIN_TAB[TrainingTab]
    APP --> SCENE[SceneBuilderTab]
    APP --> ANALYTICS[AnalyticsTab]
    APP --> ECHO_PANEL[EchoFloatingPanel]

    PROJ --> PROJ_SUB[project/<br/>NewProjectForm<br/>StorylineSection<br/>WorldSettingsPanel<br/>GenerationStylePanel]
    CHAR --> CHAR_SUB[characters/<br/>CharacterCard<br/>CharacterFilters<br/>DesignPromptEditor]
    REVIEW --> PEND_SUB[pending/<br/>ImageApprovalModal<br/>ImageCard<br/>ImageReassignModal<br/>PendingFilters<br/>ProjectCharacterGrid<br/>ReplenishmentPanel]
    TRAIN_TAB --> TRAIN_SUB[training/<br/>ConfirmDialog]
    SCENE --> SCENE_SUB[scenes/<br/>SceneLibraryView<br/>SceneEditorView<br/>GenerationMonitorView<br/>ImagePickerModal<br/>ShotDetailsPanel<br/>EpisodeView]

    REVIEW --> IDP[ImageDetailPanel]
```

## API Endpoint Groups

| Package | Prefix | Routes | Key Files |
|---------|--------|--------|-----------|
| story | `/projects`, `/storyline`, `/world-settings`, `/generation-styles` | 15 | router.py |
| visual_pipeline | `/approval/vision-review` | 5 | router.py, classification.py, vision.py, comfyui.py |
| scene_generation | `/scenes`, `/scenes/motion-presets`, `/scenes/generate-from-story` | 19 | router.py, builder.py, framepack.py, story_to_scenes.py |
| episode_assembly | `/episodes`, `/episodes/{id}/scenes`, `/episodes/{id}/assemble`, `/episodes/{id}/publish` | 10 | router.py, builder.py, publish.py |
| lora_training | `/dataset`, `/approval`, `/training`, `/gallery`, `/ingest`, `/feedback` | 32 | router.py, training_router.py, ingest_router.py, feedback.py |
| audio_composition | `/audio` | 4 | router.py |
| echo_integration | `/echo` | 4 | router.py |
| app.py | `/health`, `/gpu/status`, `/events/stats`, `/learning/*`, `/recommend/*`, `/drift`, `/quality/*`, `/correction/*`, `/replenishment/*` | 14 | — |
| **Total** | | **103** | |

## Hardware

| GPU | Service | Purpose |
|-----|---------|---------|
| NVIDIA RTX 3060 12GB | ComfyUI, Ollama | Image/video generation, Gemma3 vision review |
| AMD RX 9070 XT | Echo Brain | Embedding generation (nomic-embed-text) |

## Episode Assembly Pipeline

```mermaid
sequenceDiagram
    participant User
    participant Frontend as Vue Frontend
    participant API as Episode Router
    participant DB as PostgreSQL
    participant FFmpeg
    participant Jellyfin

    User->>Frontend: Create episode + add scenes
    Frontend->>API: POST /episodes
    API->>DB: INSERT episode

    loop Add scenes in order
        Frontend->>API: POST /episodes/{id}/scenes
        API->>DB: INSERT episode_scenes (position)
    end

    User->>Frontend: Click "Assemble"
    Frontend->>API: POST /episodes/{id}/assemble
    API->>DB: Query scene videos in position order
    API->>FFmpeg: concat demuxer (all scene MP4s)
    FFmpeg-->>API: episode_{id}.mp4
    API->>FFmpeg: Extract thumbnail (first frame)
    API->>DB: Update status=assembled, video_path, duration

    User->>Frontend: Click "Publish"
    Frontend->>API: POST /episodes/{id}/publish
    API->>API: Create Jellyfin directory structure
    API->>API: Symlink S01E01 - {Title}.mp4
    API->>Jellyfin: POST /Library/Refresh
    API->>DB: Update status=published
```

## Audio Mixing Pipeline

```mermaid
graph LR
    subgraph "Per Shot"
        DT[dialogue_text] --> TTS[Voice Synthesis<br/>per character]
    end

    subgraph "Per Scene"
        TTS --> CONCAT_D[Concat dialogue WAVs]
        MUSIC[Apple Music preview<br/>30s MP3] --> DOWNLOAD[Download preview URL]
    end

    subgraph "Mix (ffmpeg amix)"
        VIDEO[Scene video] --> MIX[ffmpeg 3-input mix]
        CONCAT_D --> |100% volume| MIX
        DOWNLOAD --> |30% volume<br/>fade in/out| MIX
        MIX --> FINAL[Final scene MP4<br/>with audio]
    end
```

## Key Design Decisions

- **Modular packages**: 6 domain packages under `packages/`, each with own router. Entry point `src/app.py` mounts all routers.
- **Auth middleware**: Local network bypass (192.168.x.x, 127.0.0.1), JWT required for external access via `X-Real-IP` header from nginx
- **Species-aware generation**: `appearance_data` in DB drives per-character negative prompts and vision review checklist
- **Non-human manual review**: Auto-approve disabled for non-human characters because vision models can't reliably distinguish stylized cartoon humans from non-human creatures
- **IP-Adapter integration**: Reference images automatically injected into ComfyUI workflow during generation
- **Background tasks**: Scene generation uses `asyncio.create_task()`, tracked in `_scene_generation_tasks` dict
- **Vault for credentials**: DB password loaded from HashiCorp Vault at startup, env var fallback
- **Character cache**: `_get_char_project_map()` cached 60 seconds to reduce DB queries
- **Ollama KEEP_ALIVE=5m**: Models auto-unload after 5 minutes to free GPU memory for ComfyUI generation
- **FramePack memory**: `gpu_memory_preservation=6.0` required for RTX 3060 (3.5 causes OOM)
- **FramePack + LoRA incompatibility**: FramePack uses HunyuanVideo architecture, NOT Stable Diffusion — SD-based character LoRAs cannot be injected into video generation
- **Episode assembly**: ffmpeg concat demuxer (no re-encoding) for fast multi-scene joins
- **Jellyfin publishing**: Symlinks (not copies) to avoid doubling disk usage; S01E01 naming convention
- **Audio mixing**: Non-fatal — if TTS or music download fails, scene video is kept without audio
- **Story-to-scenes AI**: Uses Ollama gemma3:12b for scene breakdowns; structured JSON output with retry parsing
- **Motion presets**: Curated per shot type, served via API so frontend stays in sync without hardcoding
- **FastAPI route ordering**: Static paths (`/scenes/motion-presets`, `/scenes/generate-from-story`) must be registered BEFORE dynamic `{scene_id}` routes
