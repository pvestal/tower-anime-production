#!/usr/bin/env python3
"""
Netflix-Level Video Production Service for Tower Anime Production
Connects all the pieces for real episode production with:
- AnimateDiff video generation
- LoRA character consistency
- Scene-to-scene transitions
- Episode compilation
- Audio integration
- Batch processing
- Quality control
"""

import os
import time
import json
import uuid
import asyncio
import logging
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
import requests
import aiohttp
import psycopg2
from psycopg2.extras import RealDictCursor
from fastapi import HTTPException

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
COMFYUI_URL = "http://192.168.50.135:8188"
OUTPUT_DIR = Path("/mnt/1TB-storage/ComfyUI/output")
STORAGE_DIR = Path("/mnt/10TB1/AnimeProduction")
STORAGE_DIR.mkdir(parents=True, exist_ok=True)

class NetflixLevelVideoProducer:
    """
    Main service for Netflix-quality anime production
    """

    def __init__(self):
        self.comfyui_url = COMFYUI_URL
        self.db_config = {
            'host': '192.168.50.135',
            'database': 'tower_consolidated',
            'user': 'patrick',
            'password': 'tower_echo_brain_secret_key_2025'
        }

    def get_db_connection(self):
        """Get database connection"""
        return psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)

    async def create_animatediff_video_workflow(
        self,
        prompt: str,
        character_lora: Optional[str] = None,
        duration: float = 5.0,
        resolution: str = "1920x1080",
        style: str = "anime"
    ) -> Dict[str, Any]:
        """
        Create AnimateDiff workflow for video generation with character consistency
        """
        width, height = map(int, resolution.split('x'))
        frames = int(duration * 24)  # 24 FPS
        seed = int(time.time()) % 2147483647

        # Base workflow with AnimateDiff-Evolved
        workflow = {
            # Checkpoint loader
            "1": {
                "class_type": "CheckpointLoaderSimple",
                "inputs": {
                    "ckpt_name": "AOM3A1B.safetensors"  # High-quality anime model
                }
            },

            # AnimateDiff model loader
            "2": {
                "class_type": "ADE_AnimateDiffLoaderGen1",
                "inputs": {
                    "model_name": "mm-Stabilized_high.pth",
                    "beta_schedule": "sqrt_linear (AnimateDiff)"
                }
            },

            # Context options for better temporal coherence
            "3": {
                "class_type": "ADE_AnimateDiffUniformContextOptions",
                "inputs": {
                    "context_length": 16,
                    "context_stride": 1,
                    "context_overlap": 4,
                    "context_schedule": "uniform",
                    "closed_loop": False
                }
            },

            # Apply AnimateDiff to model
            "4": {
                "class_type": "ADE_ApplyAnimateDiffModel",
                "inputs": {
                    "motion_model": ["2", 0],
                    "model": ["1", 0],
                    "context_options": ["3", 0]
                }
            },

            # Use evolved sampling
            "5": {
                "class_type": "ADE_UseEvolvedSampling",
                "inputs": {
                    "m_models": ["4", 0],
                    "model": ["1", 0]
                }
            }
        }

        # Add LoRA if character specified
        model_connection = ["5", 0]
        clip_connection = ["1", 1]

        if character_lora:
            workflow["6"] = {
                "class_type": "LoraLoader",
                "inputs": {
                    "lora_name": character_lora,
                    "strength_model": 0.8,
                    "strength_clip": 0.8,
                    "model": ["5", 0],
                    "clip": ["1", 1]
                }
            }
            model_connection = ["6", 0]
            clip_connection = ["6", 1]

        # Positive prompt
        positive_node = str(len(workflow) + 1)
        workflow[positive_node] = {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": f"{prompt}, masterpiece, best quality, {style}, high resolution, detailed animation",
                "clip": clip_connection
            }
        }

        # Negative prompt
        negative_node = str(len(workflow) + 1)
        workflow[negative_node] = {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "low quality, worst quality, static, still frame, blurry, pixelated, bad animation",
                "clip": clip_connection
            }
        }

        # Latent batch for video frames
        latent_node = str(len(workflow) + 1)
        workflow[latent_node] = {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": width,
                "height": height,
                "batch_size": frames
            }
        }

        # KSampler
        sampler_node = str(len(workflow) + 1)
        workflow[sampler_node] = {
            "class_type": "KSampler",
            "inputs": {
                "seed": seed,
                "steps": 25,
                "cfg": 7.5,
                "sampler_name": "dpmpp_2m",
                "scheduler": "karras",
                "denoise": 1.0,
                "model": model_connection,
                "positive": [positive_node, 0],
                "negative": [negative_node, 0],
                "latent_image": [latent_node, 0]
            }
        }

        # VAE Decode
        vae_node = str(len(workflow) + 1)
        workflow[vae_node] = {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": [sampler_node, 0],
                "vae": ["1", 2]
            }
        }

        # Video output
        timestamp = int(time.time())
        video_node = str(len(workflow) + 1)
        workflow[video_node] = {
            "class_type": "VHS_VideoCombine",
            "inputs": {
                "images": [vae_node, 0],
                "frame_rate": 24,
                "loop_count": 0,
                "filename_prefix": f"anime_scene_{timestamp}",
                "format": "video/h264-mp4",
                "crf": 18,
                "save_output": True
            }
        }

        return {
            "workflow": workflow,
            "expected_output": f"/mnt/1TB-storage/ComfyUI/output/anime_scene_{timestamp}_00001_.mp4",
            "timestamp": timestamp
        }

    async def generate_scene_video(
        self,
        scene_id: int,
        prompt: str,
        character_lora: Optional[str] = None,
        duration: float = 30.0
    ) -> Dict[str, Any]:
        """
        Generate video for a single scene with character consistency
        """
        logger.info(f"üé¨ Generating video for scene {scene_id}: {prompt[:50]}...")

        try:
            # Create workflow
            workflow_data = await self.create_animatediff_video_workflow(
                prompt=prompt,
                character_lora=character_lora,
                duration=duration,
                resolution="1920x1080"
            )

            # Submit to ComfyUI
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.comfyui_url}/prompt",
                    json={"prompt": workflow_data["workflow"]}
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        prompt_id = result.get("prompt_id")

                        if prompt_id:
                            logger.info(f"‚úÖ Scene {scene_id} queued with ID: {prompt_id}")

                            # Wait for completion
                            video_path = await self._wait_for_video_completion(
                                prompt_id,
                                workflow_data["expected_output"],
                                timeout=300  # 5 minutes
                            )

                            return {
                                "scene_id": scene_id,
                                "prompt_id": prompt_id,
                                "video_path": video_path,
                                "duration": duration,
                                "status": "completed" if video_path else "failed"
                            }
                        else:
                            raise ValueError("No prompt_id returned from ComfyUI")
                    else:
                        error_text = await response.text()
                        raise ValueError(f"ComfyUI error {response.status}: {error_text}")

        except Exception as e:
            logger.error(f"‚ùå Scene {scene_id} generation failed: {e}")
            return {
                "scene_id": scene_id,
                "status": "failed",
                "error": str(e)
            }

    async def _wait_for_video_completion(
        self,
        prompt_id: str,
        expected_path: str,
        timeout: int = 300
    ) -> Optional[str]:
        """
        Wait for video generation to complete
        """
        start_time = time.time()

        while time.time() - start_time < timeout:
            try:
                # Check if file exists
                if Path(expected_path).exists():
                    logger.info(f"‚úÖ Video completed: {expected_path}")
                    return expected_path

                # Check ComfyUI history
                async with aiohttp.ClientSession() as session:
                    async with session.get(f"{self.comfyui_url}/history/{prompt_id}") as response:
                        if response.status == 200:
                            history = await response.json()

                            if prompt_id in history:
                                status = history[prompt_id].get("status", {})
                                if status.get("completed"):
                                    # Check for actual file with pattern matching
                                    pattern = expected_path.replace("_00001_", "_*")
                                    files = list(Path(expected_path).parent.glob(Path(pattern).name))
                                    if files:
                                        actual_path = str(files[0])
                                        logger.info(f"‚úÖ Video found: {actual_path}")
                                        return actual_path
                                elif "error" in status:
                                    logger.error(f"‚ùå ComfyUI error: {status['error']}")
                                    return None

                # Progress update
                elapsed = time.time() - start_time
                logger.info(f"‚è≥ Waiting for completion... {elapsed:.0f}s elapsed")
                await asyncio.sleep(10)

            except Exception as e:
                logger.warning(f"Error checking completion: {e}")
                await asyncio.sleep(10)

        logger.error(f"‚ùå Timeout waiting for {prompt_id}")
        return None

    async def create_scene_transitions(
        self,
        from_scene_info: Dict[str, Any],
        to_scene_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Create transition video between two scenes
        """
        logger.info(f"üé≠ Creating transition: {from_scene_info.get('name', 'Scene')} ‚Üí {to_scene_info.get('name', 'Scene')}")

        # Build transition prompt based on scene context
        transition_prompt = self._build_transition_prompt(from_scene_info, to_scene_info)

        # Generate short transition (2 seconds)
        transition_result = await self.generate_scene_video(
            scene_id=f"transition_{from_scene_info['id']}_to_{to_scene_info['id']}",
            prompt=transition_prompt,
            duration=2.0
        )

        return transition_result

    def _build_transition_prompt(
        self,
        from_scene: Dict[str, Any],
        to_scene: Dict[str, Any]
    ) -> str:
        """
        Build intelligent transition prompt based on scene context
        """
        from_type = from_scene.get("type", "unknown")
        to_type = to_scene.get("type", "unknown")

        # Contextual transitions
        if from_type == "dialogue" and to_type == "action":
            return "cinematic camera movement, smooth zoom out from close-up to wide shot, dynamic transition, anime style"
        elif from_type == "action" and to_type == "dialogue":
            return "cinematic camera movement, smooth zoom in from wide shot to close-up, calming transition, anime style"
        elif from_type == "interior" and to_type == "exterior":
            return "smooth transition from inside to outside, camera movement through window or door, anime style"
        elif from_type == "night" and to_type == "day":
            return "time-lapse transition from night to day, lighting change, smooth temporal transition, anime style"
        else:
            return "smooth cinematic transition, fluid camera movement, seamless scene change, anime style"

    async def compile_episode(
        self,
        episode_id: str,
        scenes: List[Dict[str, Any]],
        include_transitions: bool = True,
        add_audio: bool = True
    ) -> Dict[str, Any]:
        """
        Compile multiple scenes into a complete episode
        """
        logger.info(f"üéûÔ∏è Compiling episode {episode_id} with {len(scenes)} scenes")

        try:
            compiled_segments = []
            total_duration = 0

            # Process each scene
            for i, scene in enumerate(scenes):
                logger.info(f"Processing scene {i+1}/{len(scenes)}: {scene.get('description', 'Unknown')}")

                # Generate scene video if needed
                if not scene.get("video_path"):
                    character_lora = self._get_character_lora_for_scene(scene)

                    scene_result = await self.generate_scene_video(
                        scene_id=scene["id"],
                        prompt=scene["description"],
                        character_lora=character_lora,
                        duration=scene.get("duration", 30.0)
                    )

                    if scene_result["status"] == "completed":
                        scene["video_path"] = scene_result["video_path"]
                    else:
                        logger.error(f"‚ùå Scene {scene['id']} generation failed")
                        continue

                compiled_segments.append({
                    "type": "scene",
                    "path": scene["video_path"],
                    "duration": scene.get("duration", 30.0),
                    "scene_id": scene["id"]
                })
                total_duration += scene.get("duration", 30.0)

                # Add transition if not the last scene
                if include_transitions and i < len(scenes) - 1:
                    next_scene = scenes[i + 1]
                    transition_result = await self.create_scene_transitions(scene, next_scene)

                    if transition_result["status"] == "completed":
                        compiled_segments.append({
                            "type": "transition",
                            "path": transition_result["video_path"],
                            "duration": 2.0,
                            "transition_id": transition_result["scene_id"]
                        })
                        total_duration += 2.0

            # Stitch all segments together
            final_video_path = await self._stitch_video_segments(
                compiled_segments,
                episode_id,
                total_duration
            )

            # Add audio if requested
            if add_audio and final_video_path:
                final_video_path = await self._add_episode_audio(
                    final_video_path,
                    total_duration,
                    episode_id
                )

            # Save episode info to database
            await self._save_episode_to_database(
                episode_id,
                final_video_path,
                total_duration,
                len(scenes),
                compiled_segments
            )

            return {
                "episode_id": episode_id,
                "status": "completed",
                "video_path": final_video_path,
                "total_duration": total_duration,
                "scenes_count": len(scenes),
                "segments_count": len(compiled_segments),
                "file_size_mb": self._get_file_size_mb(final_video_path) if final_video_path else 0
            }

        except Exception as e:
            logger.error(f"‚ùå Episode compilation failed: {e}")
            return {
                "episode_id": episode_id,
                "status": "failed",
                "error": str(e)
            }

    def _get_character_lora_for_scene(self, scene: Dict[str, Any]) -> Optional[str]:
        """
        Get appropriate LoRA for character consistency
        """
        characters = scene.get("characters", [])
        if characters:
            # For now, use first character - later expand to multi-character LoRA
            character_name = characters[0].lower().replace(" ", "_")
            return f"{character_name}_lora.safetensors"
        return None

    async def _stitch_video_segments(
        self,
        segments: List[Dict[str, Any]],
        episode_id: str,
        total_duration: float
    ) -> Optional[str]:
        """
        Stitch video segments using FFmpeg
        """
        logger.info(f"üé¨ Stitching {len(segments)} segments for episode {episode_id}")

        try:
            # Create concat file
            concat_file = STORAGE_DIR / f"episode_{episode_id}_concat.txt"

            with open(concat_file, 'w') as f:
                for segment in segments:
                    if segment["path"] and Path(segment["path"]).exists():
                        f.write(f"file '{segment['path']}'\n")
                    else:
                        logger.warning(f"Missing segment: {segment}")

            # Output path
            timestamp = int(time.time())
            output_path = STORAGE_DIR / f"episode_{episode_id}_{timestamp}.mp4"

            # FFmpeg command for high-quality stitching
            cmd = [
                "ffmpeg", "-y",
                "-f", "concat",
                "-safe", "0",
                "-i", str(concat_file),
                "-c:v", "libx264",
                "-crf", "18",
                "-preset", "slow",
                "-pix_fmt", "yuv420p",
                "-movflags", "+faststart",
                str(output_path)
            ]

            logger.info(f"Running FFmpeg: {' '.join(cmd[:8])}...")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)  # 30 min timeout

            if result.returncode == 0:
                logger.info(f"‚úÖ Episode stitched successfully: {output_path}")
                return str(output_path)
            else:
                logger.error(f"‚ùå FFmpeg error: {result.stderr}")
                return None

        except Exception as e:
            logger.error(f"‚ùå Video stitching failed: {e}")
            return None

    async def _add_episode_audio(
        self,
        video_path: str,
        duration: float,
        episode_id: str
    ) -> str:
        """
        Add background music and sound effects
        """
        logger.info(f"üéµ Adding audio to episode {episode_id}")

        try:
            # For now, add a simple background track
            # Later: integrate with Apple Music service for dynamic music selection
            output_path = video_path.replace(".mp4", "_with_audio.mp4")

            cmd = [
                "ffmpeg", "-y",
                "-i", video_path,
                "-filter_complex", "[0:v]volume=0.8[v];[0:a]volume=1.0[a]",
                "-map", "[v]",
                "-map", "[a]",
                "-c:v", "copy",
                "-c:a", "aac",
                str(output_path)
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)

            if result.returncode == 0:
                logger.info(f"‚úÖ Audio added: {output_path}")
                return output_path
            else:
                logger.warning(f"Audio addition failed: {result.stderr}")
                return video_path

        except Exception as e:
            logger.warning(f"Audio addition error: {e}")
            return video_path

    async def _save_episode_to_database(
        self,
        episode_id: str,
        video_path: Optional[str],
        duration: float,
        scenes_count: int,
        segments_info: List[Dict[str, Any]]
    ):
        """
        Save episode information to database
        """
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()

            # Update or insert episode record
            cursor.execute("""
                INSERT INTO episodes (id, title, status, duration, video_path, scenes, metadata, updated_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (id) DO UPDATE SET
                    status = EXCLUDED.status,
                    duration = EXCLUDED.duration,
                    video_path = EXCLUDED.video_path,
                    scenes = EXCLUDED.scenes,
                    metadata = EXCLUDED.metadata,
                    updated_at = NOW()
            """, (
                episode_id,
                f"Episode {episode_id}",
                "completed" if video_path else "failed",
                duration,
                video_path,
                json.dumps(segments_info),
                json.dumps({
                    "production_method": "netflix_level_pipeline",
                    "scenes_count": scenes_count,
                    "generation_timestamp": time.time()
                })
            ))

            conn.commit()
            conn.close()

            logger.info(f"‚úÖ Episode {episode_id} saved to database")

        except Exception as e:
            logger.error(f"‚ùå Database save failed: {e}")

    def _get_file_size_mb(self, path: str) -> float:
        """Get file size in MB"""
        try:
            return os.path.getsize(path) / (1024 * 1024)
        except:
            return 0.0

    async def generate_neon_tokyo_episode(self) -> Dict[str, Any]:
        """
        Test case: Generate complete 3-scene Neon Tokyo Nights episode
        """
        logger.info("üåÉ Starting Neon Tokyo Nights episode generation...")

        # Define test scenes
        scenes = [
            {
                "id": 7,
                "name": "Night Race",
                "description": "High-speed motorcycle chase through neon-lit Tokyo streets at midnight, cyberpunk atmosphere, dramatic lighting",
                "duration": 30.0,
                "type": "action",
                "characters": ["Luna", "Rider"]
            },
            {
                "id": 8,
                "name": "Luna's Lab",
                "description": "Luna working on holographic displays in her high-tech laboratory, analyzing data, futuristic technology",
                "duration": 30.0,
                "type": "dialogue",
                "characters": ["Luna"]
            },
            {
                "id": 5,
                "name": "Boardroom",
                "description": "Corporate boardroom meeting with city skyline view, business discussion, dramatic shadows",
                "duration": 30.0,
                "type": "dialogue",
                "characters": ["CEO", "Luna"]
            }
        ]

        # Generate complete episode
        result = await self.compile_episode(
            episode_id="neon_tokyo_nights_test",
            scenes=scenes,
            include_transitions=True,
            add_audio=True
        )

        logger.info("üé¨ Neon Tokyo Nights episode generation complete!")
        return result

# Global instance
netflix_producer = NetflixLevelVideoProducer()

if __name__ == "__main__":
    """
    Test the Netflix-level video production system
    """
    async def main():
        # Test full episode generation
        result = await netflix_producer.generate_neon_tokyo_episode()

        if result["status"] == "completed":
            print("üéâ SUCCESS! Netflix-level episode generated!")
            print(f"üìÅ Video: {result['video_path']}")
            print(f"‚è±Ô∏è Duration: {result['total_duration']:.1f}s")
            print(f"üìä Scenes: {result['scenes_count']}")
            print(f"üíæ Size: {result['file_size_mb']:.1f}MB")
        else:
            print("‚ùå Episode generation failed:")
            print(f"Error: {result.get('error', 'Unknown error')}")

    asyncio.run(main())