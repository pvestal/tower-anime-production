#!/usr/bin/env python3
"""
LTX LoRA Training Pipeline for Custom Action/NSFW Content
Train custom LoRAs on LTX 2B base model for specific movements and styles

Training Timeline:
1. Data Preparation (Day 1-2)
   - Collect video samples of desired actions
   - Extract frames at consistent intervals
   - Caption frames with desired attributes

2. Training Phase (Day 3-5)
   - Use existing NSFW LoRAs as style base
   - Fine-tune on specific actions (fighting, dancing, intimate scenes)
   - Test generations between epochs

3. Production Integration (Day 6-7)
   - Merge trained LoRAs into pipeline
   - Test with character-specific LoRAs (Mei, etc.)
   - Deploy to production workflow
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
import subprocess
import asyncio
import aiohttp

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LTXLoRATrainer:
    """Train custom LoRAs for LTX 2B video model"""

    def __init__(self):
        self.base_model = "ltxv-2b-fp8.safetensors"  # 4GB model for 12GB VRAM
        self.training_dir = Path("/mnt/1TB-storage/ltx_training")
        self.output_dir = Path("/mnt/1TB-storage/models/loras")
        self.comfyui_url = "http://localhost:8188"

        # Training categories
        self.training_categories = {
            'action': {
                'martial_arts': 'fighting poses, kicks, punches, dynamic movement',
                'dancing': 'sensual dancing, rhythmic movement, fluid motion',
                'athletics': 'running, jumping, stretching, yoga poses'
            },
            'intimate': {
                'romantic': 'kissing, embracing, intimate touching',
                'explicit': 'sexual poses, orgasm expressions, nude motion',
                'fetish': 'specific positions, BDSM, role-play scenarios'
            },
            'anime_specific': {
                'transformation': 'magical girl transformation, power-up sequences',
                'combat': 'anime-style fighting, special attacks, energy effects',
                'emotion': 'exaggerated expressions, anime reactions, comedic takes'
            }
        }

        # Existing LoRAs to build upon
        self.base_loras = {
            'motion': 'LTX2_SS_Motion_7K.safetensors',
            'nsfw': 'SexGod_Nudity_LTX2_v1_5.safetensors',
            'kiss': 'kiss_ltx2_lora.safetensors',
            'prone': 'prone_face_cam_v0_2.safetensors'
        }

    def prepare_training_data(self, category: str, subcategory: str) -> Dict:
        """Prepare training data for specific action category"""

        logger.info(f"üìÅ Preparing training data for {category}/{subcategory}")

        training_config = {
            'model': self.base_model,
            'base_lora': self.base_loras.get('motion'),  # Start with motion LoRA
            'category': category,
            'subcategory': subcategory,
            'prompt_template': self.training_categories[category][subcategory],
            'training_steps': 1000,
            'batch_size': 1,  # Limited by 12GB VRAM
            'learning_rate': 1e-5,
            'resolution': '512x384',  # Optimized for 12GB
            'frames': 49,  # 2 seconds at 24fps
            'save_every': 250  # Save checkpoints
        }

        # Create training directory
        train_path = self.training_dir / f"{category}_{subcategory}"
        train_path.mkdir(parents=True, exist_ok=True)

        # Save config
        config_path = train_path / "training_config.json"
        with open(config_path, 'w') as f:
            json.dump(training_config, f, indent=2)

        logger.info(f"‚úÖ Training config saved to {config_path}")

        return training_config

    def create_training_script(self, config: Dict) -> str:
        """Generate training script for LoRA"""

        script = f"""#!/bin/bash
# LTX LoRA Training Script
# Category: {config['category']}/{config['subcategory']}

export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Training parameters
MODEL_PATH="/mnt/1TB-storage/ComfyUI/models/checkpoints/{config['model']}"
BASE_LORA="/mnt/1TB-storage/models/loras/{config['base_lora']}"
OUTPUT_NAME="{config['category']}_{config['subcategory']}_lora"

# Use kohya_ss for LoRA training (optimized for low VRAM)
python3 train_network.py \\
    --pretrained_model_name_or_path="$MODEL_PATH" \\
    --network_module="networks.lora" \\
    --network_dim=32 \\
    --network_alpha=16 \\
    --resolution={config['resolution']} \\
    --batch_size={config['batch_size']} \\
    --max_train_steps={config['training_steps']} \\
    --learning_rate={config['learning_rate']} \\
    --lr_scheduler="cosine_with_restarts" \\
    --optimizer_type="AdamW8bit" \\
    --mixed_precision="fp16" \\
    --gradient_checkpointing \\
    --xformers \\
    --save_every_n_steps={config['save_every']} \\
    --save_model_as="safetensors" \\
    --output_dir="/mnt/1TB-storage/models/loras" \\
    --output_name="$OUTPUT_NAME" \\
    --train_data_dir="{self.training_dir}/{config['category']}_{config['subcategory']}/images" \\
    --caption_extension=".txt" \\
    --enable_bucket \\
    --min_bucket_reso=256 \\
    --max_bucket_reso=768 \\
    --bucket_reso_steps=64

echo "‚úÖ Training complete! LoRA saved as $OUTPUT_NAME.safetensors"
"""

        script_path = self.training_dir / f"train_{config['category']}_{config['subcategory']}.sh"
        with open(script_path, 'w') as f:
            f.write(script)

        os.chmod(script_path, 0o755)
        logger.info(f"üìù Training script created: {script_path}")

        return str(script_path)

    async def test_trained_lora(self, lora_path: str, test_prompt: str) -> Optional[str]:
        """Test newly trained LoRA with generation"""

        logger.info(f"üß™ Testing LoRA: {Path(lora_path).name}")

        workflow = {
            "1": {
                "class_type": "UNETLoader",
                "inputs": {"unet_name": self.base_model}
            },
            "2": {
                "class_type": "VAELoader",
                "inputs": {"vae_name": "ltx_vae.safetensors"}
            },
            "3": {
                "class_type": "CLIPLoader",
                "inputs": {
                    "clip_name": "t5xxl_fp8_e4m3fn.safetensors",
                    "type": "ltxv"
                }
            },
            "4": {
                "class_type": "LoraLoader",
                "inputs": {
                    "lora_name": Path(lora_path).name,
                    "strength_model": 1.0,
                    "strength_clip": 1.0,
                    "model": ["1", 0],
                    "clip": ["3", 0]
                }
            },
            "5": {
                "class_type": "CLIPTextEncode",
                "inputs": {
                    "text": test_prompt,
                    "clip": ["4", 1]
                }
            },
            "6": {
                "class_type": "EmptyLTXVLatentVideo",
                "inputs": {
                    "width": 512,
                    "height": 384,
                    "length": 49,
                    "batch_size": 1
                }
            },
            "7": {
                "class_type": "KSampler",
                "inputs": {
                    "seed": 42,
                    "steps": 15,
                    "cfg": 3.5,
                    "sampler_name": "euler",
                    "scheduler": "simple",
                    "positive": ["5", 0],
                    "negative": ["5", 0],
                    "latent_image": ["6", 0],
                    "model": ["4", 0],
                    "denoise": 1.0
                }
            },
            "8": {
                "class_type": "VAEDecode",
                "inputs": {
                    "samples": ["7", 0],
                    "vae": ["2", 0]
                }
            },
            "9": {
                "class_type": "VHS_VideoCombine",
                "inputs": {
                    "images": ["8", 0],
                    "frame_rate": 24,
                    "format": "video/mp4",
                    "filename_prefix": f"test_{Path(lora_path).stem}",
                    "save_output": True
                }
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.comfyui_url}/prompt",
                json={"prompt": workflow}
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    logger.info(f"‚úÖ Test generation submitted: {result.get('prompt_id')}")
                    return result.get('prompt_id')
                else:
                    logger.error(f"‚ùå Test generation failed: {await response.text()}")
                    return None

def create_training_timeline():
    """Create training timeline and schedule"""

    timeline = """
üóìÔ∏è LTX LoRA TRAINING TIMELINE (7 Days)
=====================================

Day 1-2: Data Collection & Preparation
---------------------------------------
‚ñ° Collect reference videos for each action category
  - Martial arts scenes (10-20 clips)
  - Dance sequences (10-20 clips)
  - Intimate scenes (10-20 clips)
‚ñ° Extract frames (8 fps = 192 frames from 24fps video)
‚ñ° Auto-caption with BLIP/Gemma vision
‚ñ° Manual caption refinement for accuracy

Day 3-4: Initial Training Runs
-------------------------------
‚ñ° Morning: Train 'martial_arts' LoRA (4 hours)
‚ñ° Afternoon: Train 'dancing' LoRA (4 hours)
‚ñ° Evening: Test generations, adjust parameters
‚ñ° Overnight: Train 'intimate' LoRAs (8 hours)

Day 5: Refinement & Combination
--------------------------------
‚ñ° Test LoRA combinations (character + action)
‚ñ° Fine-tune underperforming LoRAs
‚ñ° Create merged LoRAs for complex actions
‚ñ° Document optimal strength settings

Day 6: Integration Testing
---------------------------
‚ñ° Test with Tokyo Debt Desire characters
‚ñ° Verify VRAM usage stays under 12GB
‚ñ° Create production workflows
‚ñ° Batch generate test videos

Day 7: Production Deployment
-----------------------------
‚ñ° Move trained LoRAs to production
‚ñ° Update pipeline configurations
‚ñ° Create usage documentation
‚ñ° Set up automated generation queues

VRAM OPTIMIZATION TIPS:
- Train with batch_size=1
- Use gradient checkpointing
- Enable xformers
- Use 8-bit Adam optimizer
- Resolution: 512x384 max
- Clear VRAM between training runs
"""

    print(timeline)

    # Save timeline
    timeline_path = Path("/opt/tower-anime-production/production/LTX_TRAINING_TIMELINE.md")
    with open(timeline_path, 'w') as f:
        f.write(timeline)

    logger.info(f"üìÖ Timeline saved to {timeline_path}")

async def main():
    """Main training pipeline execution"""

    trainer = LTXLoRATrainer()

    # Create timeline
    create_training_timeline()

    # Example: Prepare training for martial arts
    config = trainer.prepare_training_data('action', 'martial_arts')

    # Create training script
    script_path = trainer.create_training_script(config)

    logger.info("\n" + "="*60)
    logger.info("üöÄ LTX LoRA Training Pipeline Ready!")
    logger.info("="*60)
    logger.info("\nüìã Next Steps:")
    logger.info("1. Collect training videos and place in:")
    logger.info(f"   {trainer.training_dir}/action_martial_arts/images/")
    logger.info("\n2. Create caption files (.txt) for each video frame")
    logger.info("\n3. Run training script:")
    logger.info(f"   bash {script_path}")
    logger.info("\n4. Test trained LoRA:")
    logger.info("   python3 /opt/tower-anime-production/production/ltx_lora_training_pipeline.py --test")

    # Check if we have training data ready
    train_path = trainer.training_dir / "action_martial_arts/images"
    if train_path.exists() and any(train_path.iterdir()):
        logger.info("\n‚úÖ Training data detected! Ready to start training.")
    else:
        logger.info("\n‚ö†Ô∏è  No training data found. Add videos/images first.")

if __name__ == "__main__":
    asyncio.run(main())